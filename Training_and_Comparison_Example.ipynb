{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Comparing RL Agents (PPO vs. DQN)\n",
    "\n",
    "This notebook provides a complete, end-to-end example of using the `rl_trading_project` framework with a robust data pipeline. We will perform the following steps:\n",
    "\n",
    "1.  **Data Ingestion:** Create a high-performance DuckDB database from raw, gzipped CSV files. This simulates a real-world scenario where you have large amounts of historical data.\n",
    "2.  **Data Loading & Preparation:** Load the prepared data into a Pandas DataFrame, ready for our RL environments.\n",
    "3.  **Train a PPO Agent:** Train a Proximal Policy Optimization (PPO) agent to manage a **multi-asset portfolio**. PPO is ideal for this task due to its ability to handle continuous, multi-dimensional action spaces.\n",
    "4.  **Train a DQN Agent:** Train a Dueling Deep Q-Network (DQN) agent on a **single-asset** task. We do this to highlight the strengths of DQN in discrete action spaces and show how different agents are suited to different problems.\n",
    "5.  **Backtesting & Comparison:** Evaluate both trained agents on out-of-sample data and compare their performance metrics and equity curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Ingestion with DuckDB\n",
    "\n",
    "First, we need data. We'll start by ingesting the `RAW_DIR` directory filled with monthly `csv.gz` files for multiple assets using our `ingest_raw_data_to_duckdb` function to build a persistent, columnar database named `market_data.duckdb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import duckdb\n",
    "import shutil\n",
    "\n",
    "from rl_trading_project.data.duckdb_loader import ingest_raw_data_to_duckdb\n",
    "\n",
    "RAW_DIR = '../AlphaVantage Data/raw'\n",
    "DB_PATH = 'market_data.duckdb'\n",
    "\n",
    "ingest_summary = ingest_raw_data_to_duckdb(raw_dir=RAW_DIR, db_path=DB_PATH, source_timezone='UTC')\n",
    "print(\"\\n--- Ingestion Summary ---\")\n",
    "print(ingest_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load Data and Set Up Environments\n",
    "\n",
    "With our database created, we can now easily query the data we need for our training and testing periods. We'll load all the data and preprocess to make sure all assets have enough history to create a portfolio. 2/3rd is used for training and 1/3rd for backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the database and load all data into pandas\n",
    "con = duckdb.connect(DB_PATH, read_only=True)\n",
    "portfolio_df = con.execute(\"SELECT * FROM ohlcv ORDER BY timestamp, asset\").fetchdf()\n",
    "con.close()\n",
    "\n",
    "# Convert timestamp to timezone-aware and set the multi-index required by PortfolioEnv\n",
    "portfolio_df['timestamp'] = pd.to_datetime(portfolio_df['timestamp']).dt.tz_convert('UTC')\n",
    "portfolio_df = portfolio_df.set_index(['timestamp', 'asset'])\n",
    "\n",
    "print(f\"Loaded {len(portfolio_df)} total rows for assets: {portfolio_df.index.get_level_values('asset').unique().tolist()}\")\n",
    "print(\"Data Head:\")\n",
    "print(portfolio_df.head())\n",
    "\n",
    "# Visualize the data\n",
    "fig, ax = plt.subplots(figsize=(15, 7))\n",
    "for asset in portfolio_df.index.get_level_values('asset').unique():\n",
    "    asset_prices = portfolio_df.xs(asset, level='asset')['close']\n",
    "    ax.plot(asset_prices, label=asset)\n",
    "ax.set_title('Loaded Asset Price History')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Price')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Data Filtering and Splitting ---\n",
    "# In a real-world scenario, you first filter for assets with sufficient history.\n",
    "# We will demonstrate this by filtering for assets with at least 23 months of data.\n",
    "MIN_MONTHS = 23\n",
    "month_counts = portfolio_df.reset_index().groupby('asset')['timestamp'].apply(lambda x: (x.max() - x.min()).days / 30.44)\n",
    "assets_with_enough_data = month_counts[month_counts >= MIN_MONTHS].index.tolist()\n",
    "\n",
    "print(f\"Assets with >= {MIN_MONTHS} months of data: {assets_with_enough_data}\")\n",
    "\n",
    "# assets_with_enough_data = ['AAPL', 'AMZN', 'MSFT', 'NVDA', 'QQQ']\n",
    "# Filter the main DataFrame to only include these assets\n",
    "portfolio_df = portfolio_df[portfolio_df.index.get_level_values('asset').isin(assets_with_enough_data)]\n",
    "\n",
    "# CRITICAL: Check if any assets remain after filtering before proceeding.\n",
    "if portfolio_df.empty:\n",
    "    raise ValueError(\"No assets met the minimum data requirement. Cannot proceed with training.\")\n",
    "\n",
    "# Get unique timestamps for the filtered set of assets\n",
    "unique_timestamps = portfolio_df.index.get_level_values('timestamp').unique().sort_values()\n",
    "\n",
    "# Calculate the split point (2/3 for training, 1/3 for testing)\n",
    "split_index = int(len(unique_timestamps) * (2/3))\n",
    "split_date = unique_timestamps[split_index]\n",
    "\n",
    "print(f\"Total unique timestamps: {len(unique_timestamps)}\")\n",
    "print(f\"Calculated split date for 2/3 train, 1/3 test: {split_date}\")\n",
    "\n",
    "# Split the DataFrame based on the calculated timestamp\n",
    "train_df = portfolio_df[portfolio_df.index.get_level_values('timestamp') < split_date]\n",
    "test_df = portfolio_df[portfolio_df.index.get_level_values('timestamp') >= split_date]\n",
    "\n",
    "print(f\"\\nTraining data from {train_df.index.get_level_values('timestamp').min()} to {train_df.index.get_level_values('timestamp').max()}\")\n",
    "print(f\"Testing data from {test_df.index.get_level_values('timestamp').min()} to {test_df.index.get_level_values('timestamp').max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train the PPO Agent for Multi-Asset Portfolio Management\n",
    "\n",
    "PPO is perfectly suited for our `PortfolioEnv` because its action space is a continuous vector representing the target allocation for each asset. We will train it on our multi-assset portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Import the environment factory from our utility file ---\n",
    "from ppo_training_utils import make_env_fn\n",
    "\n",
    "# --- Import other necessary libraries ---\n",
    "from rl_trading_project.agents import PPOAgent\n",
    "from rl_trading_project.utils import ExperimentLogger\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# --- CRITICAL: Use the official Gymnasium Asynchronous Vectorized Environment ---\n",
    "import gymnasium as gym\n",
    "\n",
    "# --- High-Performance PPO Setup ---\n",
    "SEED = 42\n",
    "WINDOW_SIZE = 30\n",
    "# --- NEW: Use all available CPU cores for true parallel execution ---\n",
    "NUM_ENVS = os.cpu_count() or 4\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {DEVICE} with {NUM_ENVS} parallel environments.\")\n",
    "\n",
    "\n",
    "# --- Create the Asynchronous Vectorized Environment ---\n",
    "# We pass the *imported* make_env_fn to the vectorizer.\n",
    "env_fns = [make_env_fn(train_df, WINDOW_SIZE, seed=SEED + i) for i in range(NUM_ENVS)]\n",
    "vec_env = gym.vector.AsyncVectorEnv(env_fns)\n",
    "\n",
    "\n",
    "# --- Agent and Logger Setup ---\n",
    "ppo_logger = ExperimentLogger(base_dir='runs/notebook_runs', exp_name='PPO_Parallel_RiskShaped_Notebook')\n",
    "ppo_agent = PPOAgent(\n",
    "    obs_dim=vec_env.observation_space.shape[1],  # Note the shape is (num_envs, obs_dim)\n",
    "    action_dim=vec_env.action_space.shape[1], # Note the shape is (num_envs, action_dim)\n",
    "    lr=3e-4,\n",
    "    epochs=10,\n",
    "    minibatch_size=128,\n",
    "    entropy_coef=0.01,\n",
    "    seed=SEED,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# --- High-Performance PPO Training Loop with Reward Shaping ---\n",
    "TRAIN_STEPS = 50000\n",
    "ROLLOUT_LEN = 512\n",
    "TERMINATION_PENALTY = 500.0\n",
    "LEVERAGE_PENALTY_COEF = 0.1\n",
    "\n",
    "print(\"Starting PARALLEL PPO training in Jupyter Notebook...\")\n",
    "obs, _ = vec_env.reset(seed=SEED)\n",
    "trajectories = []\n",
    "total_steps_done = 0\n",
    "\n",
    "pbar = tqdm(total=TRAIN_STEPS, desc=\"Training PPO\", unit=\"step\")\n",
    "\n",
    "while total_steps_done < TRAIN_STEPS:\n",
    "    # Rollout Phase\n",
    "    for _ in range(ROLLOUT_LEN):\n",
    "        actions, logps, values = [], [], []\n",
    "        for i in range(NUM_ENVS):\n",
    "            action, logp, value = ppo_agent.act(obs[i], deterministic=False)\n",
    "            actions.append(action)\n",
    "            logps.append(logp)\n",
    "            values.append(value)\n",
    "\n",
    "        next_obs, rewards, terminated, truncated, infos = vec_env.step(actions)\n",
    "        \n",
    "        final_infos = infos.get(\"final_info\", [None] * NUM_ENVS)\n",
    "        leverage_values = np.array([info.get('leverage', 0.0) if info else 0.0 for info in final_infos])\n",
    "        margin_called_flags = np.array([info.get('margin_called', False) if info else False for info in final_infos])\n",
    "        leverage_penalties = LEVERAGE_PENALTY_COEF * leverage_values\n",
    "        termination_penalties = TERMINATION_PENALTY * margin_called_flags\n",
    "        \n",
    "        shaped_rewards = rewards - leverage_penalties - termination_penalties\n",
    "        dones = np.logical_or(terminated, truncated)\n",
    "\n",
    "        for i in range(NUM_ENVS):\n",
    "            trajectories.append({\n",
    "                'obs': obs[i], 'act': actions[i], 'rew': shaped_rewards[i],\n",
    "                'done': dones[i], 'logp': logps[i], 'value': values[i]\n",
    "            })\n",
    "\n",
    "        obs = next_obs\n",
    "        total_steps_done += NUM_ENVS\n",
    "    \n",
    "    steps_to_update = total_steps_done - pbar.n\n",
    "    pbar.update(steps_to_update)\n",
    "\n",
    "    # Update Phase\n",
    "    stats = ppo_agent.update(trajectories)\n",
    "    trajectories.clear()\n",
    "    ppo_logger.log_metrics(stats, step=total_steps_done)\n",
    "    \n",
    "    pbar.set_postfix({\n",
    "        \"policy_loss\": f\"{stats.get('policy_loss', 0):.4f}\",\n",
    "        \"value_loss\": f\"{stats.get('value_loss', 0):.4f}\",\n",
    "    })\n",
    "\n",
    "pbar.close()\n",
    "# --- NEW: It's critical to close the vectorized environment to terminate the child processes ---\n",
    "vec_env.close()\n",
    "print(\"\\nParallel PPO Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NEW: Visualize Training Metrics ---\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_df = pd.read_csv(ppo_logger.metrics_file)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "# Plot Losses\n",
    "axes[0].plot(metrics_df['step'], metrics_df['policy_loss'], label='Policy Loss')\n",
    "axes[0].plot(metrics_df['step'], metrics_df['value_loss'], label='Value Loss')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('PPO Training Losses')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot Gradient Norms\n",
    "axes[1].plot(metrics_df['step'], metrics_df['policy_grad_norm'], label='Policy Grad Norm')\n",
    "axes[1].plot(metrics_df['step'], metrics_df['value_grad_norm'], label='Value Grad Norm')\n",
    "axes[1].set_xlabel('Training Steps')\n",
    "axes[1].set_ylabel('Gradient Norm')\n",
    "axes[1].set_title('PPO Gradient Norms')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Train the DQN Agent for Single-Asset Trading\n",
    "\n",
    "Our `DuelingDQNAgent` is a value-based agent designed for problems with a **discrete action space**. It outputs a Q-value for each possible action (e.g., full short, half short, hold, half long, full long). This is fundamentally different from PPO, which can output a continuous action vector.\n",
    "\n",
    "Therefore, we cannot directly apply our DQN agent to the multi-asset `PortfolioEnv`. Instead, we will train it on a simplified, **single-asset** version of the environment using only the `QQQ` data. This is a common and powerful use case for DQN-style agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_trading_project.agents import DuelingDQNAgent\n",
    "from rl_trading_project.envs import TradingEnv, GymWrapper\n",
    "\n",
    "# --- DQN Setup ---\n",
    "# Filter the training data for our single asset\n",
    "qqq_train_df = train_df.xs('QQQ', level='asset').reset_index()\n",
    "\n",
    "# Create a single-asset environment\n",
    "dqn_env = TradingEnv(\n",
    "    df=qqq_train_df,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    initial_balance=100_000,\n",
    "    max_position=100.0, # Max position in units of the asset\n",
    "    commission=0.0005\n",
    ")\n",
    "dqn_wrapped_env = GymWrapper(dqn_env)\n",
    "\n",
    "# Instantiate the DQN agent\n",
    "dqn_agent = DuelingDQNAgent(\n",
    "    obs_dim=dqn_wrapped_env.observation_space.shape[0],\n",
    "    action_bins=11, # Discretize action into 11 bins from -1 (full short) to +1 (full long)\n",
    "    lr=5e-4,\n",
    "    buffer_size=100_000,\n",
    "    batch_size=128,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "# --- DQN Training Loop ---\n",
    "print(\"\\nStarting DQN training...\")\n",
    "obs, _ = dqn_wrapped_env.reset(options={'start_index': WINDOW_SIZE})\n",
    "\n",
    "for step in range(TRAIN_STEPS):\n",
    "    action = dqn_agent.act(obs, deterministic=False)\n",
    "    next_obs, reward, terminated, truncated, info = dqn_wrapped_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    dqn_agent.add_experience(obs, action, reward, next_obs, done)\n",
    "    stats = dqn_agent.update(sync_freq=200)\n",
    "    \n",
    "    obs = next_obs\n",
    "    if done:\n",
    "        obs, _ = dqn_wrapped_env.reset(options={'start_index': WINDOW_SIZE})\n",
    "        \n",
    "    if (step + 1) % 2000 == 0:\n",
    "         print(f\"Step {step+1}/{TRAIN_STEPS}, Loss={stats.get('loss'):.4f}, Epsilon={stats.get('eps'):.2f}\")\n",
    "\n",
    "print(\"DQN Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# --- Part 5a: Save Trained Agents to Disk ---\n",
    "# We create a directory to store the model weights with descriptive names.\n",
    "# This prevents overwriting previous results and helps with experiment tracking.\n",
    "\n",
    "# --- NEW: Create a unique timestamped directory for this training run ---\n",
    "run_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_dir = f\"saved_models/run_{run_timestamp}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"Models for this run will be saved in: {save_dir}\")\n",
    "\n",
    "# --- NEW: More descriptive filenames ---\n",
    "# For PPO, we can include key hyperparameters in the name.\n",
    "ppo_run_name = f\"PPO_MultiAsset_LR{ppo_agent.policy_optimizer.defaults['lr']}_Rollout{ROLLOUT_LEN}\"\n",
    "PPO_SAVE_PATH = os.path.join(save_dir, ppo_run_name)\n",
    "\n",
    "# For DQN, the action space discretization is a key parameter.\n",
    "dqn_run_name = f\"DQN_SingleAsset_Bins{dqn_agent.action_bins}\"\n",
    "DQN_SAVE_PATH = os.path.join(save_dir, dqn_run_name)\n",
    "\n",
    "\n",
    "print(f\"\\nSaving PPO agent to {PPO_SAVE_PATH}...\")\n",
    "# The .save() method is part of the base Agent class and handles creating the directory\n",
    "# and saving the necessary model files (e.g., policy.pth, value.pth).\n",
    "ppo_agent.save(PPO_SAVE_PATH)\n",
    "\n",
    "print(f\"Saving DQN agent to {DQN_SAVE_PATH}...\")\n",
    "# Similarly, the DQN agent saves its Q-network and target network weights.\n",
    "dqn_agent.save(DQN_SAVE_PATH)\n",
    "\n",
    "print(\"\\nAgents saved successfully.\")\n",
    "\n",
    "# --- NEW: Save the paths for the next cell ---\n",
    "# This makes loading much easier and less prone to typos.\n",
    "# We'll store them in a dictionary for clarity.\n",
    "saved_model_paths = {\n",
    "    'ppo': PPO_SAVE_PATH,\n",
    "    'dqn': DQN_SAVE_PATH\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5b: Load Agents and Run Backtests\n",
    "\n",
    "Now for the moment of truth. To simulate a real-world workflow where training and evaluation are separate, we will first load our saved agents from disk. Then, we will use the `Backtester` module to run both of our trained agents on the out-of-sample test data. For the evaluation, we always use `deterministic=True` to make the agent exploit its learned policy without random exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Backtesting and Comparison\n",
    "\n",
    "Now for the moment of truth. We will use the `Backtester` module to run both of our trained agents on the out-of-sample test data (the 3rd month). For the evaluation, we always use `deterministic=True` to make the agent exploit its learned policy without random exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from rl_trading_project.agents import PPOAgent, DuelingDQNAgent\n",
    "from rl_trading_project.envs import PortfolioEnv, TradingEnv, GymWrapper\n",
    "from rl_trading_project.trainers import Backtester, compare_strategies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Locate or Discover Saved Model Paths ---\n",
    "# This logic makes the cell runnable in a new session.\n",
    "\n",
    "try:\n",
    "    # First, try to use the 'saved_model_paths' dict if it exists in the current session.\n",
    "    if 'saved_model_paths' in locals() and isinstance(saved_model_paths, dict):\n",
    "        print(\"Using model paths from the current training session.\")\n",
    "        PPO_SAVE_PATH = saved_model_paths['ppo']\n",
    "        DQN_SAVE_PATH = saved_model_paths['dqn']\n",
    "    else:\n",
    "        raise NameError # Force fallback to file system search\n",
    "\n",
    "except NameError:\n",
    "    print(\"Could not find model paths in session. Searching the file system for the latest run...\")\n",
    "    base_dir = 'saved_models'\n",
    "    \n",
    "    # Find the most recent 'run_*' directory\n",
    "    list_of_runs = glob.glob(os.path.join(base_dir, 'run_*'))\n",
    "    if not list_of_runs:\n",
    "        raise FileNotFoundError(f\"No run directories found in '{base_dir}'. Please run the training and saving cell first.\")\n",
    "    \n",
    "    latest_run_dir = max(list_of_runs, key=os.path.getctime)\n",
    "    print(f\"Found latest run directory: {latest_run_dir}\")\n",
    "\n",
    "    # Find the PPO and DQN model paths within the latest run directory\n",
    "    ppo_paths = glob.glob(os.path.join(latest_run_dir, 'PPO_*'))\n",
    "    dqn_paths = glob.glob(os.path.join(latest_run_dir, 'DQN_*'))\n",
    "\n",
    "    if not ppo_paths or not dqn_paths:\n",
    "        raise FileNotFoundError(f\"Could not find both a PPO and a DQN model in '{latest_run_dir}'.\")\n",
    "        \n",
    "    PPO_SAVE_PATH = ppo_paths[0]\n",
    "    DQN_SAVE_PATH = dqn_paths[0]\n",
    "\n",
    "\n",
    "# --- 2. Re-create Environments and Load Agents ---\n",
    "print(f\"\\nLoading PPO model from: {PPO_SAVE_PATH}\")\n",
    "print(f\"Loading DQN model from: {DQN_SAVE_PATH}\")\n",
    "\n",
    "SEED = 42\n",
    "WINDOW_SIZE = 30\n",
    "\n",
    "print(\"\\nRe-creating environment templates to define agent architectures...\")\n",
    "ppo_template_env = GymWrapper(PortfolioEnv(df=test_df, window_size=WINDOW_SIZE))\n",
    "qqq_test_df = test_df.xs('QQQ', level='asset').reset_index()\n",
    "dqn_template_env = GymWrapper(TradingEnv(df=qqq_test_df, window_size=WINDOW_SIZE))\n",
    "\n",
    "ppo_agent_loaded = PPOAgent(\n",
    "    obs_dim=ppo_template_env.observation_space.shape[0],\n",
    "    action_dim=ppo_template_env.action_space.shape[0],\n",
    "    seed=SEED\n",
    ")\n",
    "ppo_agent_loaded.load(PPO_SAVE_PATH)\n",
    "\n",
    "dqn_agent_loaded = DuelingDQNAgent(\n",
    "    obs_dim=dqn_template_env.observation_space.shape[0],\n",
    "    action_bins=11,\n",
    "    seed=SEED\n",
    ")\n",
    "dqn_agent_loaded.load(DQN_SAVE_PATH)\n",
    "\n",
    "print(\"\\nAgents loaded and ready for backtesting.\")\n",
    "\n",
    "\n",
    "# --- 3. PPO Backtest (Multi-Asset) ---\n",
    "def ppo_policy_fn(obs, t):\n",
    "    action, _, _ = ppo_agent_loaded.act(obs, deterministic=True)\n",
    "    return action\n",
    "\n",
    "ppo_test_env_factory = lambda: GymWrapper(PortfolioEnv(df=test_df, window_size=WINDOW_SIZE, initial_balance=100_000))\n",
    "\n",
    "print(\"\\nRunning PPO backtest...\")\n",
    "ppo_backtester = Backtester(ppo_test_env_factory, start_index=WINDOW_SIZE)\n",
    "max_ppo_steps = len(test_df.index.get_level_values('timestamp').unique()) - WINDOW_SIZE - 1\n",
    "ppo_results = ppo_backtester.run(ppo_policy_fn, max_steps=max_ppo_steps)\n",
    "\n",
    "\n",
    "# --- 4. DQN Backtest (Single-Asset on QQQ) ---\n",
    "def dqn_policy_fn(obs, t):\n",
    "    action = dqn_agent_loaded.act(obs, deterministic=True)\n",
    "    return action\n",
    "\n",
    "dqn_test_env_factory = lambda: GymWrapper(TradingEnv(df=qqq_test_df, window_size=WINDOW_SIZE, initial_balance=100_000))\n",
    "\n",
    "print(\"Running DQN backtest...\")\n",
    "dqn_backtester = Backtester(dqn_test_env_factory, start_index=WINDOW_SIZE)\n",
    "max_dqn_steps = len(qqq_test_df) - WINDOW_SIZE - 1\n",
    "dqn_results = dqn_backtester.run(dqn_policy_fn, max_steps=max_dqn_steps)\n",
    "\n",
    "\n",
    "# --- 5. Comparison and Plotting ---\n",
    "comparison = compare_strategies({\n",
    "    'PPO_MultiAsset': ppo_results,\n",
    "    'DQN_SingleAsset': dqn_results\n",
    "})\n",
    "\n",
    "print(\"\\n--- Backtest Comparison Summary ---\")\n",
    "summary_df = pd.DataFrame(comparison).T[['total_return', 'sharpe_ratio', 'max_drawdown', 'end_value']]\n",
    "summary_df['total_return'] = summary_df['total_return'].apply(lambda x: f\"{x:.2%}\")\n",
    "summary_df['max_drawdown'] = summary_df['max_drawdown'].apply(lambda x: f\"{x:.2%}\")\n",
    "print(summary_df)\n",
    "\n",
    "ppo_history_df = pd.DataFrame(ppo_results['history'])\n",
    "dqn_history_df = pd.DataFrame(dqn_results['history'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.plot(ppo_history_df['total_value'], label='PPO (Multi-Asset Portfolio)', lw=2)\n",
    "ax.plot(dqn_history_df['total_value'], label='DQN (Single-Asset: QQQ)', lw=2)\n",
    "ax.set_title('Agent Equity Curves (Out-of-Sample)')\n",
    "ax.set_xlabel('Test Steps')\n",
    "ax.set_ylabel('Portfolio Value ($)')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the full workflow: from raw data ingestion to training multiple, distinct RL agents and comparing their performance. \n",
    "\n",
    "The **PPO agent** successfully learned a policy to manage a portfolio of correlated assets, leveraging its ability to output continuous allocation vectors. \n",
    "\n",
    "The **DQN agent**, while not suitable for the multi-asset task in its current form, proved effective for a single-asset trading problem with a discretized set of actions (buy/sell/hold decisions). \n",
    "\n",
    "This highlights a key principle in applied RL: choosing the right agent architecture for the specific problem and action space is critical for success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
